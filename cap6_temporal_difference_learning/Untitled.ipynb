{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gym_gridworld:gridworld-v4', size=3, player_pos=[0,0], goal_pos=[[2,2]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabularTDZero(policy=[0.25,0.25,0.25,0.25], alpha=0.1, gamma=1):\n",
    "    V = [0 for i in env.observation_space]\n",
    "    \n",
    "    for i in range(100000):\n",
    "        obs0 = env.reset()\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.random.choice([UP, RIGHT, DOWN, LEFT], p=policy)\n",
    "            obs1, reward, done, info = env.step(action)\n",
    "            \n",
    "            obs0_i = env.observation_space.index(obs0)\n",
    "            obs1_i = env.observation_space.index(obs1)\n",
    "            \n",
    "            V[obs0_i] = V[obs0_i] + alpha * (reward + gamma * V[obs1_i] - V[obs0_i])\n",
    "            \n",
    "            obs0 = obs1\n",
    "        \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tabularTDZero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-26.63526579, -24.01310281, -22.12160586],\n",
       "       [-24.71451945, -21.64036123, -15.52603753],\n",
       "       [-22.73708552, -16.42409977,   0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a).reshape(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gym_gridworld:gridworld-v4', size=4, player_pos=[0,3], goal_pos=[[3,3], [0,0]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,  -8.51834845, -17.51567378, -19.7675834 ],\n",
       "       [-13.35316172, -15.54478301, -17.99881873, -16.48236428],\n",
       "       [-17.36659171, -18.55989591, -16.90376001,  -9.96660682],\n",
       "       [-20.70701166, -19.89462409, -14.49348776,   0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tabularTDZero()\n",
    "np.array(a).reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(alpha=0.1, epsilon=0.1, gamma=1):\n",
    "    Q = {a: [0 for s in env.observation_space] for a in [UP, RIGHT, DOWN, LEFT]}\n",
    "    \n",
    "    for i in range(10000):\n",
    "        obs0 = env.reset()\n",
    "        obs0_i = env.observation_space.index(obs0)\n",
    "        action0 = max({a: Q[a][obs0_i] for a in Q}, key=Q.get)\n",
    "        action0 = np.random.choice([action0, UP, RIGHT, DOWN, LEFT], p=[1-epsilon,epsilon/4,epsilon/4,epsilon/4,epsilon/4])\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            obs1, reward, done, info = env.step(action0)\n",
    "            \n",
    "            obs1_i = env.observation_space.index(obs1)\n",
    "            action1 = max({a: Q[a][obs1_i] for a in Q}, key=Q.get)\n",
    "            action1 = np.random.choice([action1, UP, RIGHT, DOWN, LEFT], p=[1-epsilon,epsilon/4,epsilon/4,epsilon/4,epsilon/4])\n",
    "            \n",
    "            Q[action0][obs0_i] = Q[action0][obs0_i] + alpha * (reward + gamma * Q[action1][obs1_i] - Q[action0][obs0_i])\n",
    "            \n",
    "            obs0_i = obs1_i\n",
    "            action0 = action1\n",
    "            \n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sarsa = sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs0 = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs0_i = env.observation_space.index(obs0)\n",
    "action = max({a: a_sarsa[a][obs0_i] for a in a_sarsa}, key=a_sarsa.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs0, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
