{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gym_gridworld:gridworld-v4', size=3, player_pos=[0,0], goal_pos=[[2,2]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular TD(0) Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabularTDZero(policy=[0.25,0.25,0.25,0.25], alpha=0.1, gamma=1):\n",
    "    V = [0 for i in env.observation_space]\n",
    "    \n",
    "    for i in range(100000):\n",
    "        obs0 = env.reset()\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.random.choice([UP, RIGHT, DOWN, LEFT], p=policy)\n",
    "            obs1, reward, done, info = env.step(action)\n",
    "            \n",
    "            obs0_i = env.observation_space.index(obs0)\n",
    "            obs1_i = env.observation_space.index(obs1)\n",
    "            \n",
    "            V[obs0_i] = V[obs0_i] + alpha * (reward + gamma * V[obs1_i] - V[obs0_i])\n",
    "            \n",
    "            obs0 = obs1\n",
    "        \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tabularTDZero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-26.63526579, -24.01310281, -22.12160586],\n",
       "       [-24.71451945, -21.64036123, -15.52603753],\n",
       "       [-22.73708552, -16.42409977,   0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a).reshape(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gym_gridworld:gridworld-v4', size=4, player_pos=[0,3], goal_pos=[[3,3], [0,0]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,  -8.51834845, -17.51567378, -19.7675834 ],\n",
       "       [-13.35316172, -15.54478301, -17.99881873, -16.48236428],\n",
       "       [-17.36659171, -18.55989591, -16.90376001,  -9.96660682],\n",
       "       [-20.70701166, -19.89462409, -14.49348776,   0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tabularTDZero()\n",
    "np.array(a).reshape(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(alpha=0.1, epsilon=0.1, gamma=1):\n",
    "    Q = [[0 for s in env.observation_space] for a in [UP, RIGHT, DOWN, LEFT]]\n",
    "    \n",
    "    # Loop for each episode\n",
    "    for i in range(100000):\n",
    "        \n",
    "        # Initialize S\n",
    "        obs0 = env.reset()\n",
    "        \n",
    "        # Choose A from S using policy derived from Q (e-greedy)\n",
    "        obs0_i = env.observation_space.index(obs0)\n",
    "        action0 = np.argmax([Q[i][obs0_i] for i, a in enumerate(Q)])\n",
    "        action0 = np.random.choice([action0, UP, RIGHT, DOWN, LEFT], p=[1-epsilon,epsilon/4,epsilon/4,epsilon/4,epsilon/4])\n",
    "        \n",
    "        # Loop for each step of episode\n",
    "        done = False\n",
    "        while not done:\n",
    "            \n",
    "            # Take action A, observe R, S'\n",
    "            obs1, reward, done, info = env.step(action0)\n",
    "            \n",
    "            # Choose A' from S' using policy derived from Q (e-greedy)\n",
    "            obs1_i = env.observation_space.index(obs1)\n",
    "            action1 = np.argmax([Q[i][obs1_i] for i, a in enumerate(Q)])\n",
    "            action1 = np.random.choice([action1, UP, RIGHT, DOWN, LEFT], p=[1-epsilon,epsilon/4,epsilon/4,epsilon/4,epsilon/4])\n",
    "            \n",
    "            # Q(S,A) <- Q(S,A) + alpha * [ R + gamma * Q(S',A') - Q(S,A) ]\n",
    "            Q[action0][obs0_i] = Q[action0][obs0_i] + alpha * (reward + gamma * Q[action1][obs1_i] - Q[action0][obs0_i])\n",
    "            \n",
    "            #print('-------------')\n",
    "            #print(Q)\n",
    "            #print(action0, action1)\n",
    "            #print(obs1, reward, done)\n",
    "            #print(obs0_i, obs1_i)\n",
    "            #print([Q[i][obs1_i] for i, a in enumerate(Q)])\n",
    "            \n",
    "            # S <- S'; A <- A'\n",
    "            obs0_i = obs1_i\n",
    "            action0 = action1\n",
    "            \n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a_sarsa = sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.37664781 -3.47920544 -2.29884309]\n",
      " [-4.5523513  -3.46657483 -2.37272378]\n",
      " [-3.52933475 -2.26217682  0.        ]] \n",
      "\n",
      "[[-3.56206115 -2.61889128 -2.40212858]\n",
      " [-2.71338079 -1.45783181 -1.03649461]\n",
      " [-1.35573237  0.          0.        ]] \n",
      "\n",
      "[[-3.19051821 -2.33432524 -1.32731641]\n",
      " [-2.11856654 -1.27186723  0.        ]\n",
      " [-2.28946262 -1.05874868  0.        ]] \n",
      "\n",
      "[[-4.43839352 -4.53100277 -3.71898457]\n",
      " [-3.25701855 -3.37312487 -2.32795292]\n",
      " [-2.23058735 -2.32599179  0.        ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in a_sarsa:\n",
    "    print(np.array(i).reshape(3,3), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_i = env.observation_space.index(obs)\n",
    "action = np.argmax([a_sarsa[i][obs_i] for i, a in enumerate(a_sarsa)])\n",
    "\n",
    "obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gym_gridworld:gridworld-v4', size=6, player_pos=[0,5], goal_pos=[[0,0], [5,2]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sarsa = sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_i = env.observation_space.index(obs)\n",
    "action = np.argmax([a_sarsa[i][obs_i] for i, a in enumerate(a_sarsa)])\n",
    "\n",
    "obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -1.24693125 -2.41097443 -3.49867662 -4.77243523 -5.85872809]\n",
      " [ 0.         -1.59339271 -2.37346611 -3.6574015  -4.56531387 -5.81624263]\n",
      " [-1.37175487 -2.19788288 -2.86902851 -3.82142588 -4.37770954 -5.59192594]\n",
      " [-1.53791373 -1.68214213 -2.73938591 -3.19930353 -3.40756721 -4.45467371]\n",
      " [-1.11044734 -0.95762391 -1.90131037 -1.97831293 -2.74765522 -3.23400147]\n",
      " [-0.57649626 -0.1         0.         -1.27022949 -1.71309716 -2.35122328]] \n",
      "\n",
      "[[ 0.         -2.38561587 -3.85641088 -4.75724554 -5.92371145 -5.67670893]\n",
      " [-2.52549927 -3.34753948 -4.52525529 -5.53741274 -6.63062934 -6.50718162]\n",
      " [-2.01028442 -2.39470529 -3.1957332  -3.80658166 -5.10852887 -5.23903127]\n",
      " [-1.59402409 -1.65003505 -2.21308574 -2.65770994 -3.85637069 -4.07784783]\n",
      " [-1.03764744 -0.87842335 -1.37269851 -2.31463406 -2.69562521 -3.08544257]\n",
      " [-0.61257951  0.          0.         -1.4129204  -2.12213072 -2.30126012]] \n",
      "\n",
      "[[ 0.         -2.51306115 -3.30156807 -4.39297987 -5.33715649 -6.41396196]\n",
      " [-2.45226203 -3.21916374 -3.17522635 -4.15974402 -5.17920244 -6.01259152]\n",
      " [-1.80542202 -2.26816382 -2.16328337 -3.19708649 -4.16707381 -5.02872334]\n",
      " [-1.43680234 -1.60478893 -1.07407817 -2.09879318 -3.10475316 -4.05274544]\n",
      " [-1.00663961 -0.86491483  0.         -1.05441109 -2.11064808 -3.0880279 ]\n",
      " [-0.665357    0.          0.         -0.61784078 -1.54693174 -2.20950659]] \n",
      "\n",
      "[[ 0.          0.         -1.12467915 -2.25524106 -3.52755999 -4.71219185]\n",
      " [-1.35515202 -1.05339188 -2.4729939  -3.36431991 -4.3952088  -5.34216734]\n",
      " [-2.01927125 -2.29097805 -2.74962033 -3.18854582 -4.18181657 -5.02976893]\n",
      " [-1.52408894 -1.63533459 -1.84142329 -2.25428725 -3.16015855 -4.06681026]\n",
      " [-1.08568877 -0.96338179 -1.30011613 -1.28971454 -2.22365754 -3.0819966 ]\n",
      " [-0.69070331 -0.1         0.          0.         -1.0695766  -2.16575053]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in a_sarsa:\n",
    "    print(np.array(i).reshape(6,6), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearning(alpha=0.1, epsilon=0.1, gamma=1):\n",
    "    Q = [[0 for s in env.observation_space] for a in [UP, RIGHT, DOWN, LEFT]]\n",
    "    \n",
    "    # Loop for each episode\n",
    "    for i in range(100000):\n",
    "        \n",
    "        # Initialize S\n",
    "        obs = env.reset()\n",
    "        \n",
    "        # Loop for each step of episode\n",
    "        done = False\n",
    "        while not done:\n",
    "            \n",
    "            # Choose A from S using policy derived from Q (e-greedy)\n",
    "            obs_i = env.observation_space.index(obs)\n",
    "            action = np.argmax([Q[i][obs_i] for i, a in enumerate(Q)])\n",
    "            action = np.random.choice([action, UP, RIGHT, DOWN, LEFT], p=[1-epsilon,epsilon/4,epsilon/4,epsilon/4,epsilon/4])\n",
    "            \n",
    "            # Take action A, observe R, S'\n",
    "            obs1, reward, done, info = env.step(action)            \n",
    "            \n",
    "            # Q(S,A) <- Q(S,A) + alpha * [ R + gamma * max_a( Q(S',a) ) - Q(S,A) ]\n",
    "            obs1_i = env.observation_space.index(obs1)\n",
    "            max_Q = np.max([Q[i][obs1_i] for i, a in enumerate(Q)])\n",
    "            Q[action][obs_i] = Q[action][obs_i] + alpha * (reward + gamma * max_Q - Q[action][obs_i])\n",
    "            \n",
    "            # S <- S'\n",
    "            obs = obs1\n",
    "            \n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_qLearning = qLearning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-1e01602fb650>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, a, b, c = env.step(DOWN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_i = env.observation_space.index(obs)\n",
    "action = np.argmax([a_sarsa[i][obs_i] for i, a in enumerate(a_sarsa)])\n",
    "\n",
    "obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -1.         -2.         -3.         -4.         -5.        ]\n",
      " [ 0.         -1.         -2.         -3.         -4.         -5.        ]\n",
      " [-0.99363731 -1.94984495 -2.20203959 -3.07563027 -4.04225818 -4.79581156]\n",
      " [-1.09477031 -1.42739996 -1.96936601 -2.26658759 -3.32351734 -3.84535524]\n",
      " [-0.81996824 -0.80141834 -0.73607645 -1.19720184 -2.4710588  -2.88912006]\n",
      " [-0.58870647 -0.1         0.         -0.67547431 -1.22269394 -2.16759702]] \n",
      "\n",
      "[[ 0.         -2.         -3.         -4.         -5.         -5.        ]\n",
      " [-1.53843351 -2.99884857 -3.99995611 -4.99879087 -5.99930991 -5.9996852 ]\n",
      " [-1.08448486 -2.18863461 -2.3886971  -3.37150555 -4.01548107 -4.86088101]\n",
      " [-1.14746878 -1.43122574 -1.39147487 -2.30098941 -3.11045    -3.8831233 ]\n",
      " [-0.86453188 -0.71757046 -1.12651855 -1.36280543 -2.05496167 -2.99563703]\n",
      " [-0.468559    0.          0.         -0.70773872 -1.5918233  -1.98059455]] \n",
      "\n",
      "[[ 0.         -2.         -3.         -4.         -5.         -6.        ]\n",
      " [-0.82763089 -2.91665147 -2.99938413 -3.98304785 -4.92183453 -5.6573889 ]\n",
      " [-1.07320499 -1.99573443 -1.99999919 -2.99342902 -3.95178592 -4.72272121]\n",
      " [-1.12575467 -1.34829925 -1.         -1.99792524 -2.97317658 -3.79853299]\n",
      " [-0.8503343  -0.71757046  0.         -0.99966701 -1.9907966  -2.88325054]\n",
      " [-0.49        0.          0.         -0.40951    -1.095087   -1.983637  ]] \n",
      "\n",
      "[[ 0.          0.         -1.         -2.         -3.         -4.        ]\n",
      " [-0.56953279 -1.         -2.         -3.         -4.         -5.        ]\n",
      " [-1.18595996 -1.95167505 -2.0017486  -2.99381267 -3.95198866 -4.72106106]\n",
      " [-1.09765872 -1.41657777 -1.52863862 -1.99803185 -2.97433532 -3.79766343]\n",
      " [-0.8        -0.81540511 -0.71052794 -0.99963001 -1.99086791 -2.89310008]\n",
      " [-0.5         0.          0.          0.         -0.99995002 -1.97869654]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in a_qLearning:\n",
    "    print(np.array(i).reshape(6,6), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
